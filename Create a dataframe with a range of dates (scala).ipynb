{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe with a range of dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://HQSWL-1650935.cable.comcast.com:4043\n",
       "SparkContext available as 'sc' (version = 2.4.0, master = local[*], app id = local-1562806836273)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-10 21:00:32 ERROR Shell:397 - Failed to locate the winutils binary in the hadoop binary path\r\n",
      "java.io.IOException: Could not locate executable null\\bin\\winutils.exe in the Hadoop binaries.\r\n",
      "\tat org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:379)\r\n",
      "\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:394)\r\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:387)\r\n",
      "\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)\r\n",
      "\tat org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)\r\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:273)\r\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:261)\r\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:791)\r\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:761)\r\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:634)\r\n",
      "\tat org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2422)\r\n",
      "\tat org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2422)\r\n",
      "\tat scala.Option.getOrElse(Option.scala:121)\r\n",
      "\tat org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2422)\r\n",
      "\tat org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)\r\n",
      "\tat org.apache.spark.deploy.SparkSubmit.secMgr$lzycompute$1(SparkSubmit.scala:359)\r\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$secMgr$1(SparkSubmit.scala:359)\r\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anonfun$prepareSubmitEnvironment$7.apply(SparkSubmit.scala:367)\r\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anonfun$prepareSubmitEnvironment$7.apply(SparkSubmit.scala:367)\r\n",
      "\tat scala.Option.map(Option.scala:146)\r\n",
      "\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:366)\r\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:143)\r\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)\r\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:924)\r\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:933)\r\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\n",
      "2019-07-10 21:00:33 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n",
      "2019-07-10 21:00:36 WARN  Utils:66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\r\n",
      "2019-07-10 21:00:36 WARN  Utils:66 - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\r\n",
      "2019-07-10 21:00:36 WARN  Utils:66 - Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.joda.time._\r\n",
       "day: org.joda.time.LocalDate = 2018-07-01\r\n",
       "targetDatePathYesterday: String = 2018/06/30\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.joda.time._\n",
    "\n",
    "val day = new LocalDate(\"2018-07-01\")\n",
    "val targetDatePathYesterday = day.plusDays(-1).toString(\"yyyy/MM/dd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "minutes: scala.collection.immutable.IndexedSeq[String] = Vector(2018-06-30T00:00:00.000, 2018-06-30T00:01:00.000, 2018-06-30T00:02:00.000, 2018-06-30T00:03:00.000, 2018-06-30T00:04:00.000, 2018-06-30T00:05:00.000, 2018-06-30T00:06:00.000, 2018-06-30T00:07:00.000, 2018-06-30T00:08:00.000, 2018-06-30T00:09:00.000, 2018-06-30T00:10:00.000, 2018-06-30T00:11:00.000, 2018-06-30T00:12:00.000, 2018-06-30T00:13:00.000, 2018-06-30T00:14:00.000, 2018-06-30T00:15:00.000, 2018-06-30T00:16:00.000, 2018-06-30T00:17:00.000, 2018-06-30T00:18:00.000, 2018-06-30T00:19:00.000, 2018-06-30T00:20:00.000, 2018-06-30T00:21:00.000, 2018-06-30T00:22:00.000, 2018-06-30T00:23:00.000, 2018-06-30T00:24:00.000, 2018-06-30T00:25:00.000, 2018-06-30T00:26:00.000, 2018-06-30T00:27:00.000, 2018-06-30T00:28:00.000, 2018-06-..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val minutes = for (f <- 0 to 60*24-1) yield(new LocalDateTime(targetDatePathYesterday.replace(\"/\", \"-\") + \"T00:00:00.000\").plusMinutes(f).toString)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|minutes            |\n",
      "+-------------------+\n",
      "|2018-06-30 00:00:00|\n",
      "|2018-06-30 00:01:00|\n",
      "|2018-06-30 00:02:00|\n",
      "|2018-06-30 00:03:00|\n",
      "|2018-06-30 00:04:00|\n",
      "|2018-06-30 00:05:00|\n",
      "|2018-06-30 00:06:00|\n",
      "|2018-06-30 00:07:00|\n",
      "|2018-06-30 00:08:00|\n",
      "|2018-06-30 00:09:00|\n",
      "|2018-06-30 00:10:00|\n",
      "|2018-06-30 00:11:00|\n",
      "|2018-06-30 00:12:00|\n",
      "|2018-06-30 00:13:00|\n",
      "|2018-06-30 00:14:00|\n",
      "|2018-06-30 00:15:00|\n",
      "|2018-06-30 00:16:00|\n",
      "|2018-06-30 00:17:00|\n",
      "|2018-06-30 00:18:00|\n",
      "|2018-06-30 00:19:00|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df_by_minute: org.apache.spark.sql.DataFrame = [minutes: timestamp]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_by_minute = minutes\n",
    ".toDF(\"minutes\")\n",
    ".withColumn(\"minutes\", $\"minutes\".cast(\"timestamp\"))\n",
    "\n",
    "df_by_minute.show(false)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
